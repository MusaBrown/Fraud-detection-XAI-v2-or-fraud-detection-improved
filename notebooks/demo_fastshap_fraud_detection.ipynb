{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time XAI for Credit Card Fraud Detection\n",
    "\n",
    "## FastSHAP Implementation Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Training fraud detection models (XGBoost, LightGBM, CatBoost)\n",
    "2. Training FastSHAP neural surrogate for fast explanations\n",
    "3. Benchmarking latency and fidelity\n",
    "4. Analyzing the latency-fidelity Pareto frontier\n",
    "5. Running streaming simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Import our modules\n",
    "# NOTE: This notebook uses ULB Credit Card dataset (IEEE-CIS code present but NOT used in project)
from src.data.load_datasets import ULBLoader, create_synthetic_ulb_data, temporal_split\n",
    "from src.data.preprocessing import FraudDataPreprocessor\n",
    "from src.models.train_models import FraudModelTrainer, train_multiple_models\n",
    "from src.explainers.baseline_shap import TreeSHAPExplainer, KernelSHAPExplainer\n",
    "from src.explainers.fastshap_implementation import FastSHAPExplainer, create_fastshap_from_model\n",
    "from src.explainers.lime_optimized import OptimizedLIME, benchmark_lime\n",
    "from src.explainers.fidelity_metrics import ExplanationEvaluator\n",
    "from src.evaluation.latency_benchmark import LatencyBenchmark\n",
    "from src.evaluation.pareto_analysis import ParetoAnalyzer\n",
    "from src.evaluation.stability_analysis import StabilityAnalyzer\n",
    "\n",
    "print(\"Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data (or real data if available)\n",
    "loader = IEEECISLoader()\n",
    "df = loader._create_synthetic_data(n_samples=20000, n_features=50)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['isFraud'].mean():.4f}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal split\n",
    "train_df, val_df, test_df = temporal_split(df, test_size=0.2, val_size=0.1)\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = FraudDataPreprocessor(\n",
    "    categorical_threshold=20,\n",
    "    max_categories=50,\n",
    "    scale_features=True\n",
    ")\n",
    "\n",
    "train_processed = preprocessor.fit_transform(train_df, target_col='isFraud')\n",
    "val_processed = preprocessor.transform(val_df)\n",
    "test_processed = preprocessor.transform(test_df)\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [c for c in train_processed.columns if c not in ['isFraud', 'TransactionID', 'TransactionDT']]\n",
    "\n",
    "X_train = train_processed[feature_cols].values.astype(np.float32)\n",
    "y_train = train_processed['isFraud'].values\n",
    "X_val = val_processed[feature_cols].values.astype(np.float32)\n",
    "y_val = val_processed['isFraud'].values\n",
    "X_test = test_processed[feature_cols].values.astype(np.float32)\n",
    "y_test = test_processed['isFraud'].values\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Fraud Detection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost and LightGBM\n",
    "models = train_multiple_models(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    model_types=['xgboost', 'lightgbm']\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "for name, model in models.items():\n",
    "    metrics = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\n{name.upper()} Test Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost as primary model\n",
    "primary_model = models['xgboost']\n",
    "\n",
    "# Get feature importance\n",
    "importance_df = primary_model.get_feature_importance().head(15)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, y='feature', x='importance')\n",
    "plt.title('Top 15 Feature Importances (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train FastSHAP Neural Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TreeSHAP explainer as teacher\n",
    "print(\"Creating TreeSHAP teacher...\")\n",
    "tree_explainer = TreeSHAPExplainer(primary_model.model, feature_names=feature_cols)\n",
    "tree_explainer.fit()\n",
    "print(f\"TreeSHAP expected value: {tree_explainer.expected_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for FastSHAP\n",
    "print(\"Generating training data for FastSHAP...\")\n",
    "n_train_samples = min(5000, len(X_train))\n",
    "train_indices = np.random.choice(len(X_train), n_train_samples, replace=False)\n",
    "\n",
    "train_result = tree_explainer.explain(X_train[train_indices])\n",
    "shap_train = train_result['shap_values']\n",
    "expected_value = train_result['expected_value']\n",
    "\n",
    "# Validation data\n",
    "val_result = tree_explainer.explain(X_val[:500])\n",
    "shap_val = val_result['shap_values']\n",
    "\n",
    "print(f\"SHAP train shape: {shap_train.shape}\")\n",
    "print(f\"SHAP val shape: {shap_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train FastSHAP\n",
    "print(\"Training FastSHAP surrogate...\")\n",
    "\n",
    "fastshap = FastSHAPExplainer(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dims=[256, 128, 64],\n",
    "    dropout=0.2,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "training_result = fastshap.fit(\n",
    "    X_train=X_train[train_indices],\n",
    "    shap_values_train=shap_train,\n",
    "    X_val=X_val[:500],\n",
    "    shap_values_val=shap_val,\n",
    "    expected_value=expected_value\n",
    ")\n",
    "\n",
    "print(\"FastSHAP training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "history_df = pd.DataFrame(training_result['history'])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in history_df.columns:\n",
    "    plt.plot(history_df['epoch'], history_df['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('FastSHAP Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final validation fidelity: {training_result['fidelity'].get('pearson', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on held-out test set\n",
    "n_test = min(500, len(X_test))\n",
    "test_indices = np.random.choice(len(X_test), n_test, replace=False)\n",
    "\n",
    "# Get ground truth\n",
    "tree_test_result = tree_explainer.explain(X_test[test_indices])\n",
    "true_shap_test = tree_test_result['shap_values']\n",
    "\n",
    "# Evaluate FastSHAP fidelity\n",
    "fidelity_metrics = fastshap.compute_fidelity(\n",
    "    X_test[test_indices], \n",
    "    true_shap_test,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"FastSHAP Fidelity Metrics:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in fidelity_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric:30s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fidelity: Predicted vs True SHAP values\n",
    "fastshap_test = fastshap.explain(X_test[test_indices])['shap_values']\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(true_shap_test.flatten(), fastshap_test.flatten(), alpha=0.3, s=1)\n",
    "plt.plot([true_shap_test.min(), true_shap_test.max()], \n",
    "         [true_shap_test.min(), true_shap_test.max()], \n",
    "         'r--', label='Perfect correlation')\n",
    "plt.xlabel('True SHAP Values (TreeSHAP)')\n",
    "plt.ylabel('Predicted SHAP Values (FastSHAP)')\n",
    "plt.title(f\"Fidelity: r={fidelity_metrics['pearson']:.4f}\")\n",
    "plt.legend()\n",
    "\n",
    "# Residuals\n",
    "plt.subplot(1, 3, 2)\n",
    "residuals = (fastshap_test - true_shap_test).flatten()\n",
    "plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Residual (Predicted - True)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Residual Distribution')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "# Feature importance comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "true_imp = np.abs(true_shap_test).mean(axis=0)\n",
    "pred_imp = np.abs(fastshap_test).mean(axis=0)\n",
    "plt.scatter(true_imp, pred_imp, alpha=0.6)\n",
    "plt.xlabel('True Feature Importance')\n",
    "plt.ylabel('Predicted Feature Importance')\n",
    "plt.title('Global Feature Importance')\n",
    "plt.plot([0, true_imp.max()], [0, true_imp.max()], 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Latency Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark all methods\n",
    "benchmark = LatencyBenchmark(warmup_runs=5)\n",
    "\n",
    "# Define explain functions\n",
    "def tree_shap_fn(X):\n",
    "    return {'shap_values': tree_explainer.explain(X, check_additivity=False)['shap_values']}\n",
    "\n",
    "def fastshap_fn(X):\n",
    "    return {'shap_values': fastshap.explain(X)['shap_values']}\n",
    "\n",
    "# Benchmark TreeSHAP\n",
    "benchmark.benchmark_method(\n",
    "    \"TreeSHAP (exact)\",\n",
    "    tree_shap_fn,\n",
    "    X_test[:100],\n",
    "    n_samples=50\n",
    ")\n",
    "\n",
    "# Benchmark FastSHAP\n",
    "benchmark.benchmark_method(\n",
    "    \"FastSHAP\",\n",
    "    fastshap_fn,\n",
    "    X_test[:200],\n",
    "    n_samples=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "latency_df = benchmark.get_results_df()\n",
    "display(latency_df)\n",
    "\n",
    "# Plot\n",
    "benchmark.plot_latency_distribution()\n",
    "plt.show()\n",
    "\n",
    "benchmark.plot_percentile_comparison()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pareto Frontier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pareto analysis\n",
    "pareto = ParetoAnalyzer()\n",
    "\n",
    "# Add benchmarked methods\n",
    "for result in benchmark.results:\n",
    "    # Map to fidelity scores\n",
    "    fidelity = 1.0 if 'FastSHAP' in result.method_name else 0.95\n",
    "    if 'TreeSHAP' in result.method_name:\n",
    "        fidelity = 1.0  # Ground truth\n",
    "    \n",
    "    pareto.add_point(\n",
    "        method=result.method_name,\n",
    "        latency_ms=result.p95_ms,\n",
    "        fidelity=fidelity\n",
    "    )\n",
    "\n",
    "# Add hypothetical configurations\n",
    "for nsamples in [50, 100, 200, 500]:\n",
    "    # Estimate latency (rough approximation)\n",
    "    latency = nsamples * 0.5\n",
    "    # Estimate fidelity (more samples = better fidelity)\n",
    "    fidelity = 0.7 + 0.25 * (1 - np.exp(-nsamples/200))\n",
    "    pareto.add_point(\n",
    "        method=f\"KernelSHAP (n={nsamples})\",\n",
    "        latency_ms=latency,\n",
    "        fidelity=fidelity\n",
    "    )\n",
    "\n",
    "pareto.compute_pareto_frontier()\n",
    "print(pareto.generate_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto frontier\n",
    "fig = pareto.plot_pareto_frontier(\n",
    "    latency_threshold=50,\n",
    "    fidelity_threshold=0.9,\n",
    "    title=\"Latency-Fidelity Pareto Frontier - Fraud Detection XAI\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stability under input perturbations\n",
    "stability_analyzer = StabilityAnalyzer(fastshap.explain)\n",
    "\n",
    "stability_metrics = stability_analyzer.analyze_input_perturbation(\n",
    "    X_test[:50],\n",
    "    n_perturbations=10,\n",
    "    noise_level=0.01\n",
    ")\n",
    "\n",
    "print(\"Stability Metrics:\")\n",
    "print(f\"  Mean Coefficient of Variation: {stability_metrics.mean_cv:.4f}\")\n",
    "print(f\"  Max Coefficient of Variation: {stability_metrics.max_cv:.4f}\")\n",
    "print(f\"  Perturbation Consistency: {stability_metrics.perturbation_consistency:.4f}\")\n",
    "print(f\"  Rank Correlation Mean: {stability_metrics.rank_correlation_mean:.4f}\")\n",
    "print(f\"  Sign Consistency: {stability_metrics.sign_consistency:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get explanations for a few examples\n",
    "example_indices = [0, 1, 2]\n",
    "\n",
    "for idx in example_indices:\n",
    "    x = X_test[idx]\n",
    "    y_true = y_test[idx]\n",
    "    \n",
    "    # Prediction\n",
    "    prob = primary_model.predict_proba(x.reshape(1, -1))[0]\n",
    "    \n",
    "    # Explanations\n",
    "    tree_exp = tree_explainer.explain_single(x)['shap_values']\n",
    "    fastshap_exp = fastshap.explain_single(x)['shap_values']\n",
    "    \n",
    "    print(f\"\\nExample {idx} - True: {y_true}, Predicted: {prob:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Top features\n",
    "    top_k = 5\n",
    "    top_indices = np.argsort(np.abs(fastshap_exp))[-top_k:][::-1]\n",
    "    \n",
    "    print(f\"{'Feature':<20} {'True SHAP':<15} {'FastSHAP':<15}\")\n",
    "    for i in top_indices:\n",
    "        print(f\"{feature_cols[i]:<20} {tree_exp[i]:<15.4f} {fastshap_exp[i]:<15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explanation for one example\n",
    "idx = 0\n",
    "x = X_test[idx]\n",
    "tree_exp = tree_explainer.explain_single(x)['shap_values']\n",
    "fastshap_exp = fastshap.explain_single(x)['shap_values']\n",
    "\n",
    "# Get top 10 features\n",
    "top_k = 10\n",
    "top_indices = np.argsort(np.abs(tree_exp))[-top_k:][::-1]\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'feature': [feature_cols[i] for i in top_indices],\n",
    "    'TreeSHAP': [tree_exp[i] for i in top_indices],\n",
    "    'FastSHAP': [fastshap_exp[i] for i in top_indices]\n",
    "})\n",
    "\n",
    "# Plot\n",
    "x_pos = np.arange(len(plot_df))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.barh(x_pos - width/2, plot_df['TreeSHAP'], width, label='TreeSHAP', alpha=0.8)\n",
    "ax.barh(x_pos + width/2, plot_df['FastSHAP'], width, label='FastSHAP', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(plot_df['feature'])\n",
    "ax.set_xlabel('SHAP Value')\n",
    "ax.set_title(f'SHAP Explanation Comparison - Example {idx}')\n",
    "ax.legend()\n",
    "ax.axvline(x=0, color='black', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"REAL-TIME XAI FOR FRAUD DETECTION - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Model performance\n",
    "test_metrics = primary_model.evaluate(X_test, y_test)\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  AUC-ROC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "# FastSHAP performance\n",
    "print(\"\\nFastSHAP Performance:\")\n",
    "print(f\"  Fidelity (Pearson r): {fidelity_metrics['pearson']:.4f}\")\n",
    "print(f\"  Top-10 Rank Correlation: {fidelity_metrics['spearman_topk_mean']:.4f}\")\n",
    "\n",
    "# Latency\n",
    "fastshap_latency = latency_df[latency_df['method'] == 'FastSHAP'].iloc[0]\n",
    "print(\"\\nFastSHAP Latency:\")\n",
    "print(f\"  Mean: {fastshap_latency['mean_ms']:.2f} ms\")\n",
    "print(f\"  P95: {fastshap_latency['p95_ms']:.2f} ms\")\n",
    "print(f\"  P99: {fastshap_latency['p99_ms']:.2f} ms\")\n",
    "\n",
    "# Check targets\n",
    "print(\"\\nTarget Compliance:\")\n",
    "print(f\"  P95 < 50ms: {'✓ PASS' if fastshap_latency['p95_ms'] < 50 else '✗ FAIL'} ({fastshap_latency['p95_ms']:.2f} ms)\")\n",
    "print(f\"  Fidelity > 0.90: {'✓ PASS' if fidelity_metrics['pearson'] > 0.90 else '✗ FAIL'} ({fidelity_metrics['pearson']:.4f})\")\n",
    "print(f\"  F1 > 0.95: {'✓ PASS' if test_metrics['f1'] > 0.95 else '✗ FAIL'} ({test_metrics['f1']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
